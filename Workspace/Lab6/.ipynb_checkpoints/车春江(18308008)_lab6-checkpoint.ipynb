{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab6 Navis Bayes 应用实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 切分文本成词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'Python',\n",
       " 'or',\n",
       " 'M.L.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 利用string.split()方法切分文本字符串\n",
    "mySent = \"This book is the best book on Python or M.L. I have ever laid eyes upon.\"\n",
    "mySent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'Python',\n",
       " 'or',\n",
       " 'M',\n",
       " 'L',\n",
       " 'I',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon',\n",
       " '']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 利用正则表达式切分,其中的分隔符是除单词、数字外的任意字符串\n",
    "import re\n",
    "regEx = re.compile('\\\\W+')\n",
    "listOfTokens = regEx.split(mySent)\n",
    "listOfTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'since', 'you', 'are', 'an', 'owner', 'of', 'at', 'least', 'one', 'google', 'groups', 'group', 'that', 'uses', 'the', 'customized', 'welcome', 'message', 'pages', 'or', 'files', 'we', 'are', 'writing', 'to', 'inform', 'you', 'that', 'we', 'will', 'no', 'longer', 'be', 'supporting', 'these', 'features', 'starting', 'february', '2011', 'we', 'made', 'this', 'decision', 'so', 'that', 'we', 'can', 'focus', 'on', 'improving', 'the', 'core', 'functionalities', 'of', 'google', 'groups', 'mailing', 'lists', 'and', 'forum', 'discussions', 'instead', 'of', 'these', 'features', 'we', 'encourage', 'you', 'to', 'use', 'products', 'that', 'are', 'designed', 'specifically', 'for', 'file', 'storage', 'and', 'page', 'creation', 'such', 'as', 'google', 'docs', 'and', 'google', 'sites', 'for', 'example', 'you', 'can', 'easily', 'create', 'your', 'pages', 'on', 'google', 'sites', 'and', 'share', 'the', 'site', 'http', 'www', 'google', 'com', 'support', 'sites', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '174623', 'with', 'the', 'members', 'of', 'your', 'group', 'you', 'can', 'also', 'store', 'your', 'files', 'on', 'the', 'site', 'by', 'attaching', 'files', 'to', 'pages', 'http', 'www', 'google', 'com', 'support', 'sites', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '90563', 'on', 'the', 'site', 'if', 'you', 're', 'just', 'looking', 'for', 'a', 'place', 'to', 'upload', 'your', 'files', 'so', 'that', 'your', 'group', 'members', 'can', 'download', 'them', 'we', 'suggest', 'you', 'try', 'google', 'docs', 'you', 'can', 'upload', 'files', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '50092', 'and', 'share', 'access', 'with', 'either', 'a', 'group', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '66343', 'or', 'an', 'individual', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '86152', 'assigning', 'either', 'edit', 'or', 'download', 'only', 'access', 'to', 'the', 'files', 'you', 'have', 'received', 'this', 'mandatory', 'email', 'service', 'announcement', 'to', 'update', 'you', 'about', 'important', 'changes', 'to', 'google', 'groups']\n"
     ]
    }
   ],
   "source": [
    "# 列表推导式的应用\n",
    "emailText = open('email/ham/6.txt', encoding=\n",
    " 'ISO-8859-1').read()\n",
    "listOfTokens = regEx.split(emailText)\n",
    "listOfTokens=[tok.lower() for tok in listOfTokens if len(tok) > 0]\n",
    "print(listOfTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to',\n",
       " 'flea',\n",
       " 'problems',\n",
       " 'my',\n",
       " 'how',\n",
       " 'worthless',\n",
       " 'mr',\n",
       " 'so',\n",
       " 'love',\n",
       " 'is',\n",
       " 'park',\n",
       " 'I',\n",
       " 'posting',\n",
       " 'not',\n",
       " 'buying',\n",
       " 'quit',\n",
       " 'food',\n",
       " 'dalmation',\n",
       " 'ate',\n",
       " 'dog',\n",
       " 'stop',\n",
       " 'him',\n",
       " 'steak',\n",
       " 'has',\n",
       " 'help',\n",
       " 'take',\n",
       " 'cute',\n",
       " 'please',\n",
       " 'stupid',\n",
       " 'maybe',\n",
       " 'garbage',\n",
       " 'licks']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 函数 loadDataSet()生成实验样本集\n",
    "# 函数 createVocabList()建立词汇表\n",
    "import bayes\n",
    "listOPost, listClasses = bayes.loadDataSet()\n",
    "myVocabList = bayes.createVocabList(listOPost)\n",
    "myVocabList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# 调用 setOfWords2Vec()函数生成词集向量\n",
    "# 构建 listOPost 列表 0 位置对应的词集向量\n",
    "setOfWords2Vec0 = bayes.setOfWords2Vec(myVocabList, listOPost[0])\n",
    "print(setOfWords2Vec0)\n",
    "# 构建 listOPost 列表 3 位置对应的词集向量\n",
    "setOfWords2Vec3 = bayes.setOfWords2Vec(myVocabList, listOPost[3])\n",
    "print(setOfWords2Vec3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "[0.07692308 0.07692308 0.07692308 0.15384615 0.07692308 0.03846154\n",
      " 0.07692308 0.07692308 0.07692308 0.07692308 0.03846154 0.07692308\n",
      " 0.03846154 0.03846154 0.03846154 0.03846154 0.03846154 0.07692308\n",
      " 0.07692308 0.07692308 0.07692308 0.11538462 0.07692308 0.07692308\n",
      " 0.07692308 0.03846154 0.07692308 0.07692308 0.03846154 0.03846154\n",
      " 0.03846154 0.07692308]\n",
      "[0.0952381  0.04761905 0.04761905 0.04761905 0.04761905 0.14285714\n",
      " 0.04761905 0.04761905 0.04761905 0.04761905 0.0952381  0.04761905\n",
      " 0.0952381  0.0952381  0.0952381  0.0952381  0.0952381  0.04761905\n",
      " 0.04761905 0.14285714 0.0952381  0.0952381  0.04761905 0.04761905\n",
      " 0.04761905 0.0952381  0.04761905 0.04761905 0.19047619 0.0952381\n",
      " 0.0952381  0.04761905]\n"
     ]
    }
   ],
   "source": [
    "# 测试 train()函数,返回两个概率向量和一个概率值\n",
    "# for 循环使用词向量充填 trainMat 列表\n",
    "trainMat = []\n",
    "for postinDoc in listOPost:\n",
    "    trainMat.append(bayes.bagOfWords2Vec (myVocabList, postinDoc))\n",
    "p0V, p1V, pAb = bayes.train(trainMat, listClasses)\n",
    "print(pAb)\n",
    "print(p0V)\n",
    "print(p1V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classfication error ['hommies', 'just', 'got', 'phone', 'call', 'from', 'the', 'roofer', 'they', 'will', 'come', 'and', 'spaying', 'the', 'foaming', 'today', 'will', 'dusty', 'pls', 'close', 'all', 'the', 'doors', 'and', 'windows', 'could', 'you', 'help', 'close', 'bathroom', 'window', 'cat', 'window', 'and', 'the', 'sliding', 'door', 'behind', 'the', 'don', 'know', 'how', 'can', 'those', 'cats', 'survive', 'sorry', 'for', 'any', 'inconvenience']\n",
      "classfication error ['linkedin', 'kerry', 'haloney', 'requested', 'add', 'you', 'connection', 'linkedin', 'peter', 'like', 'add', 'you', 'professional', 'network', 'linkedin', 'kerry', 'haloney']\n",
      "classfication error ['saw', 'this', 'the', 'way', 'the', 'coast', 'thought', 'might', 'like', 'hangzhou', 'huge', 'one', 'day', 'wasn', 'enough', 'but', 'got', 'glimpse', 'went', 'inside', 'the', 'china', 'pavilion', 'expo', 'pretty', 'interesting', 'each', 'province', 'has', 'exhibit']\n",
      "classfication error ['been', 'working', 'running', 'website', 'using', 'jquery', 'and', 'the', 'jqplot', 'plugin', 'not', 'too', 'far', 'away', 'from', 'having', 'prototype', 'launch', 'you', 'used', 'jqplot', 'right', 'not', 'think', 'you', 'would', 'like']\n",
      "classfication error ['peter', 'with', 'jose', 'out', 'town', 'you', 'want', 'meet', 'once', 'while', 'keep', 'things', 'going', 'and', 'some', 'interesting', 'stuff', 'let', 'know', 'eugene']\n",
      "classfication error ['this', 'mail', 'was', 'sent', 'from', 'notification', 'only', 'address', 'that', 'cannot', 'accept', 'incoming', 'mail', 'please', 'not', 'reply', 'this', 'message', 'thank', 'you', 'for', 'your', 'online', 'reservation', 'the', 'store', 'you', 'selected', 'has', 'located', 'the', 'item', 'you', 'requested', 'and', 'has', 'placed', 'hold', 'your', 'name', 'please', 'note', 'that', 'all', 'items', 'are', 'held', 'for', 'day', 'please', 'note', 'store', 'prices', 'may', 'differ', 'from', 'those', 'online', 'you', 'have', 'questions', 'need', 'assistance', 'with', 'your', 'reservation', 'please', 'contact', 'the', 'store', 'the', 'phone', 'number', 'listed', 'below', 'you', 'can', 'also', 'access', 'store', 'information', 'such', 'store', 'hours', 'and', 'location', 'the', 'web', 'http', 'www', 'borders', 'com', 'online', 'store', 'storedetailview_98']\n",
      "classfication error ['peter', 'the', 'hotels', 'are', 'the', 'ones', 'that', 'rent', 'out', 'the', 'tent', 'they', 'are', 'all', 'lined', 'the', 'hotel', 'grounds', 'much', 'for', 'being', 'one', 'with', 'nature', 'more', 'like', 'being', 'one', 'with', 'couple', 'dozen', 'tour', 'groups', 'and', 'nature', 'have', 'about', '100m', 'pictures', 'from', 'that', 'trip', 'can', 'through', 'them', 'and', 'get', 'you', 'jpgs', 'favorite', 'scenic', 'pictures', 'where', 'are', 'you', 'and', 'jocelyn', 'now', 'new', 'york', 'will', 'you', 'come', 'tokyo', 'for', 'chinese', 'new', 'year', 'perhaps', 'see', 'the', 'two', 'you', 'then', 'will', 'thailand', 'for', 'winter', 'holiday', 'see', 'mom', 'take', 'care']\n",
      "the error rate is  0.7\n"
     ]
    }
   ],
   "source": [
    "# spamTest()函数完成测试\n",
    "bayes.spamTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 操作习题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现验证极大似然估计示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29.99257394615612, 2.0223823631811446)\n"
     ]
    }
   ],
   "source": [
    "# 实现极大似然估计\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "μ = 30  # 数学期望\n",
    "σ = 2  # 方差\n",
    "x = μ + σ * np.random.randn(10000)  # 正态分布\n",
    "plt.hist(x, bins=100)  # 直方图显示\n",
    "plt.show()\n",
    "print(norm.fit(x))  # 返回极大似然估计，估计出参数约为30和2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用sklearn中BernouliNB分类该邮件数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classfication error ['yay', 'you', 'both', 'doing', 'fine', 'working', 'mba', 'design', 'strategy', 'cca', 'top', 'art', 'school', 'new', 'program', 'focusing', 'more', 'right', 'brained', 'creative', 'and', 'strategic', 'approach', 'management', 'the', 'way', 'done', 'today']\n",
      "classfication error ['yeah', 'ready', 'may', 'not', 'here', 'because', 'jar', 'jar', 'has', 'plane', 'tickets', 'germany', 'for']\n",
      "classfication error ['will', 'there', 'the', 'latest']\n",
      "the error rate is  0.3\n",
      "the error rate is  0.0\n",
      "classfication error ['home', 'based', 'business', 'opportunity', 'knocking', 'your', 'door', 'don', 'rude', 'and', 'let', 'this', 'chance', 'you', 'can', 'earn', 'great', 'income', 'and', 'find', 'your', 'financial', 'life', 'transformed', 'learn', 'more', 'here', 'your', 'success', 'work', 'from', 'home', 'finder', 'experts']\n",
      "the error rate is  0.1\n",
      "classfication error ['ryan', 'whybrew', 'commented', 'your', 'status', 'ryan', 'wrote', 'turd', 'ferguson', 'butt', 'horn']\n",
      "the error rate is  0.1\n",
      "the error rate is  0.0\n",
      "classfication error ['ryan', 'whybrew', 'commented', 'your', 'status', 'ryan', 'wrote', 'turd', 'ferguson', 'butt', 'horn']\n",
      "classfication error ['will', 'there', 'the', 'latest']\n",
      "classfication error ['yeah', 'ready', 'may', 'not', 'here', 'because', 'jar', 'jar', 'has', 'plane', 'tickets', 'germany', 'for']\n",
      "the error rate is  0.3\n",
      "the error rate is  0.0\n",
      "the error rate is  0.0\n",
      "the error rate is  0.0\n",
      "classfication error ['home', 'based', 'business', 'opportunity', 'knocking', 'your', 'door', 'don', 'rude', 'and', 'let', 'this', 'chance', 'you', 'can', 'earn', 'great', 'income', 'and', 'find', 'your', 'financial', 'life', 'transformed', 'learn', 'more', 'here', 'your', 'success', 'work', 'from', 'home', 'finder', 'experts']\n",
      "the error rate is  0.1\n",
      "average error rate: 0.09\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import numpy as np\n",
    "from bayes import textParse\n",
    "from bayes import createVocabList\n",
    "from bayes import bagOfWords2Vec\n",
    "\n",
    "def myskBernoulliNB():\n",
    "    fullTest = [];docList = [];classList= []\n",
    "    # it only 25 doc in every class\n",
    "    for i in range(1,26): \n",
    "        wordList = textParse(open('email/spam/%d.txt' % i,encoding=\"ISO-8859-1\").read())\n",
    "        docList.append(wordList)\n",
    "        fullTest.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open('email/ham/%d.txt' % i,encoding=\"ISO-8859-1\").read())\n",
    "        docList.append(wordList)\n",
    "        fullTest.extend(wordList)\n",
    "        classList.append(0)\n",
    "    # create vocabulary\n",
    "    vocabList = createVocabList(docList)   \n",
    "    trainSet = list(range(50));testSet=[]\n",
    "    # choose 10 sample to test ,it index of trainMat\n",
    "    for i in range(10):\n",
    "        randIndex = int(np.random.uniform(0,len(trainSet)))#num in 0-49\n",
    "        testSet.append(trainSet[randIndex])\n",
    "        del(trainSet[randIndex])\n",
    "    trainMat = []; trainClass = []\n",
    "    for docIndex in trainSet:\n",
    "        trainMat.append(bagOfWords2Vec(vocabList,docList[docIndex]))\n",
    "        trainClass.append(classList[docIndex])\n",
    "    clf=BernoulliNB()\n",
    "    #print(type(np.array(trainMat)))\n",
    "    clf.fit(np.array(trainMat),np.array(trainClass))    \n",
    "    errCount = 0\n",
    "    for docIndex in testSet:\n",
    "        wordVec=bagOfWords2Vec(vocabList,docList[docIndex])\n",
    "        #print(wordVec)\n",
    "        if clf.predict(np.array([wordVec])) != classList[docIndex]:\n",
    "            errCount += 1\n",
    "            print ((\"classfication error\"), docList[docIndex])\n",
    "\n",
    "    print ((\"the error rate is \") , float(errCount)/len(testSet))\n",
    "    return float(errCount)/len(testSet)\n",
    "\n",
    "ave=0\n",
    "for i in range(10):\n",
    "    ave=ave+myskBernoulliNB()\n",
    "print('average error rate:',ave/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将词集向量用 TF-IDF 词向量替代,测试分析结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/faker/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:45: DeprecationWarning: scipy.mat is deprecated and will be removed in SciPy 2.0.0, use numpy.mat instead\n",
      "/home/faker/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:46: DeprecationWarning: scipy.mat is deprecated and will be removed in SciPy 2.0.0, use numpy.mat instead\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import scipy\n",
    "def myskBernoulliNB2():\n",
    "    fullTest = [];docList = [];classList= []\n",
    "    # it only 25 doc in every class\n",
    "    for i in range(1,26): \n",
    "        wordList = open('email/spam/%d.txt' % i,encoding=\"ISO-8859-1\").read()\n",
    "        docList.append(wordList)\n",
    "        fullTest.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = open('email/ham/%d.txt' % i,encoding=\"ISO-8859-1\").read()\n",
    "        docList.append(wordList)\n",
    "        fullTest.extend(wordList)\n",
    "        classList.append(0)\n",
    "    # create vocabulary\n",
    "    vocabList = createVocabList(docList)   \n",
    "    trainSet = list(range(50));testSet=[]\n",
    "    # choose 10 sample to test ,it index of trainMat\n",
    "    for i in range(10):\n",
    "        randIndex = int(np.random.uniform(0,len(trainSet)))#num in 0-49\n",
    "        testSet.append(trainSet[randIndex])\n",
    "        del(trainSet[randIndex])\n",
    "    transformer = TfidfTransformer()\n",
    "    vectorizer = CountVectorizer()\n",
    "    #print(transformer)\n",
    "    #print(docList)\n",
    "    tfidf = transformer.fit_transform(vectorizer.fit_transform(docList)).A\n",
    "    #print(type(tfidf.A))\n",
    "    #print(tfidf.A)\n",
    "    trainmail=[tfidf[i] for i in trainSet]\n",
    "    trainClass=[classList[i] for i in trainSet]\n",
    "    testmail=[tfidf[i] for i in testSet]\n",
    "    testClass=[classList[i] for i in testSet]\n",
    "    #print(np.array(trainmail[1]))\n",
    "    #print(trainmail)\n",
    "    #trainMat.voc=vectorizer.vocabulary_\n",
    "    #print(trainMat)\n",
    "    clf=BernoulliNB()\n",
    "    #print(type(np.array(trainMat)))\n",
    "    #print(type(tfidf))\n",
    "    #print(trainmail)\n",
    "    clf.fit(scipy.mat(trainmail),trainClass)    \n",
    "    print(clf.score(scipy.mat(testmail),testClass))\n",
    "\n",
    "myskBernoulliNB2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 选择适合的模型对购买计算机示例数据集建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0], [0, 0, 0, 1], [1, 0, 0, 0], [2, 1, 0, 0], [2, 2, 1, 0], [2, 2, 1, 1], [1, 2, 1, 1], [0, 1, 0, 0], [0, 2, 1, 0], [2, 1, 1, 0], [0, 1, 1, 1], [1, 1, 0, 1], [1, 0, 1, 0], [2, 1, 0, 1]]\n",
      "[0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0]\n",
      "error rate: 0.35714285714285715\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "data81=[\"youth high no fair no\",\n",
    "\"youth high no excellent no\",\n",
    "\"middle_aged high no fair yes\",\n",
    "\"senior medium no fair yes\",\n",
    "\"senior low yes fair yes\",\n",
    "\"senior low yes excellent no\",\n",
    "\"middle_aged low yes excellent yes\",\n",
    "\"youth medium no fair no\",\n",
    "\"youth low yes fair yes\",\n",
    "\"senior medium yes fair yes\",\n",
    "\"youth medium yes excellent yes\",\n",
    "\"middle_aged medium no excellent yes\",\n",
    "\"middle_aged high yes fair yes\",\n",
    "\"senior medium no excellent no\",\n",
    "]\n",
    "n=len(data81)\n",
    "mat81=[]\n",
    "cls81=[]\n",
    "for i in range(n):\n",
    "    spl=data81[i].split()\n",
    "    tmp=[spl[j] for j in range(4)]\n",
    "    mat81.append(tmp)\n",
    "    if (spl[4]=='yes'):\n",
    "        cls81.append(1)\n",
    "    else:\n",
    "        cls81.append(0)\n",
    "for j in range(4):\n",
    "    dict={}\n",
    "    cnt=0\n",
    "    for i in range(n):\n",
    "        if not (mat81[i][j] in dict):\n",
    "            dict[mat81[i][j]]=cnt\n",
    "            cnt+=1\n",
    "    for i in range(n):\n",
    "        mat81[i][j]=dict[mat81[i][j]]\n",
    "print(mat81)\n",
    "print(cls81)\n",
    "#mat81=[[1,1],[0,1]]\n",
    "#cls81=[0,1]\n",
    "clf=MultinomialNB()\n",
    "clf.fit(np.array(mat81),np.array(cls81)) \n",
    "err=0\n",
    "tot=0\n",
    "for i in range(n):\n",
    "    tot+=1\n",
    "    if clf.predict(np.array([mat81[i]]))!=cls81[i]:\n",
    "        err+=1\n",
    "print('error rate:',err/tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
